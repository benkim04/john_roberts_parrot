{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original script by J.Tharsen 03-2025\n",
    "# modified from www.datacamp.com/tutorial/llama-3-1-rag\n",
    "\n",
    "# Install libraries (if needed)\n",
    "#!pip install langchain langchain_community langchain-openai scikit-learn langchain-ollama sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0bdb50ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "# Load documents from local files\n",
    "docs_list = []\n",
    "metadata_list = []\n",
    "source_dir = \"./cases\"\n",
    "df = pd.read_csv(\"./cases/metadata.csv\")\n",
    "\n",
    "for filename in glob.glob(source_dir + \"/*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    docs_list.append(filedata)\n",
    "    row = df[(\"./cases/\" + df['filename']) == filename]\n",
    "    if not row.empty:\n",
    "        # Extract metadata as a dictionary\n",
    "        metadata = {\n",
    "            'name': row.iloc[0]['name'],\n",
    "            'year': row.iloc[0]['year'],\n",
    "            'legal_issue': row.iloc[0]['legal_issue']\n",
    "        }\n",
    "        metadata_list.append(metadata)\n",
    "    else:\n",
    "        print(f\"Warning: No metadata found for file {filename}\")\n",
    "        metadata_list.append(None)\n",
    "\n",
    "print(str(len(docs_list)) + \" documents loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Optional arguments for the text_splitter\n",
    "#    length_function=len,\n",
    "#    is_separator_regex=False,\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.create_documents(docs_list, metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7004577c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Thompson v. United States, 604 U.S.',\n",
       " 'year': np.int64(2025),\n",
       " 'legal_issue': 'Statutory Interpretation'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first document shard\n",
    "doc_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6202ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "with open(\"secret_key.txt\", \"r\") as file:\n",
    "    my_api_key = file.read().strip()\n",
    "#my_api_key = \"your_key_here\"\n",
    "# Create a file in this directory titled \"secret_key.txt\" and add your key.\n",
    "\n",
    "# Create embeddings for documents and store them in a vector store\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=my_api_key),\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cace7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the prompt template for the LLM\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are modeled after chief justice John Roberts.\n",
    "    Use the following documents to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Answer in the style of justice John Roberts.\n",
    "    Remain brief with a maximum of 7 sentences:\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6f977a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with the chosen model, set temperature to 0\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cf4c2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain combining the prompt template and LLM\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG application class\n",
    "class RAGApplication:\n",
    "    def __init__(self, retriever, rag_chain):\n",
    "        self.retriever = retriever\n",
    "        self.rag_chain = rag_chain\n",
    "    def run(self, question):\n",
    "        # Retrieve relevant documents\n",
    "        documents = self.retriever.invoke(question)\n",
    "        for doc in documents:\n",
    "            print(doc)\n",
    "        # Extract content from retrieved documents\n",
    "        doc_texts = \"\\\\n\".join([doc.page_content for doc in documents])\n",
    "        # Get the answer from the language model\n",
    "        answer = self.rag_chain.invoke({\"question\": question, \"documents\": doc_texts})\n",
    "        return answer, doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f4164b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG application\n",
    "rag_application = RAGApplication(retriever, rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3e4fcc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Under that rule, ambiguities of all stripes trigger deference. Indeed, the Government and, seemingly, the dissent continue to defend the proposition that Chevron applies even in cases having little to do with an agency’s technical subject matter expertise. See Brief for Respondents in No. 22–1219, p. 17; post, at 10.' metadata={'id': '9582199e-b750-4576-96ad-4c0b88816c74', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "page_content='(quoting Janus v. State, County, and Municipal Employees, 585 U.S. 878, 917 (2018))—all weigh in favor of letting Chevron go.' metadata={'id': '6a424245-c91d-4e09-9777-9ef91f7e6ca6', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "page_content='those cases where it might appear to be applicable. See W. Eskridge & L. Baer, The Continuum of Deference: Supreme Court Treatment of Agency Statutory Interpretations From Chevron to Hamdan, 96 Geo. L. J. 1083, 1125 (2008). At this point, all that remains of Chevron is a decaying husk with bold pretensions.' metadata={'id': '6da5f5b4-8e97-47b8-9037-e9cfc384776e', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "page_content='This Court, for its part, has not deferred to an agency interpretation under Chevron since 2016. See Cuozzo, 579 U. S., at 280 (most recent occasion). But Chevron remains on the books. So litigants must continue to wrestle with it, and lower courts—bound by even our crumbling precedents, see Agostini v. Felton, 521 U.S. 203, 238 (1997)—understandably continue to apply it.' metadata={'id': '52b598c6-24f0-49ff-b761-b5a3f908efb1', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "Question: Should Chevron Deference remain?\n",
      "Answer: The question of whether Chevron Deference should remain is a complex one. As I have written in my dissenting opinions, the rule has become increasingly strained and its application has grown more arbitrary. The Court's own actions suggest that Chevron's time has passed; we have not deferred to an agency interpretation under Chevron since 2016. Nevertheless, it remains on the books, and litigants must continue to navigate its ambiguities. I fear that if we do not reexamine Chevron, it will become a mere relic of our judicial history, devoid of any meaningful purpose. It is time for us to reconsider whether this deference is still warranted.\n",
      "Documents: Under that rule, ambiguities of all stripes trigger deference. Indeed, the Government and, seemingly, the dissent continue to defend the proposition that Chevron applies even in cases having little to do with an agency’s technical subject matter expertise. See Brief for Respondents in No. 22–1219, p. 17; post, at 10.\\n(quoting Janus v. State, County, and Municipal Employees, 585 U.S. 878, 917 (2018))—all weigh in favor of letting Chevron go.\\nthose cases where it might appear to be applicable. See W. Eskridge & L. Baer, The Continuum of Deference: Supreme Court Treatment of Agency Statutory Interpretations From Chevron to Hamdan, 96 Geo. L. J. 1083, 1125 (2008). At this point, all that remains of Chevron is a decaying husk with bold pretensions.\\nThis Court, for its part, has not deferred to an agency interpretation under Chevron since 2016. See Cuozzo, 579 U. S., at 280 (most recent occasion). But Chevron remains on the books. So litigants must continue to wrestle with it, and lower courts—bound by even our crumbling precedents, see Agostini v. Felton, 521 U.S. 203, 238 (1997)—understandably continue to apply it.\n"
     ]
    }
   ],
   "source": [
    "# Run the RAG application\n",
    "question = \"Should Chevron Deference remain?\"\n",
    "answer, doc_texts = rag_application.run(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Documents:\", doc_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c1a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
