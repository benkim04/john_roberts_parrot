{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96a7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original script by J.Tharsen 03-2025\n",
    "# modified from www.datacamp.com/tutorial/llama-3-1-rag\n",
    "\n",
    "# Install libraries (if needed)\n",
    "#!pip install langchain langchain_community langchain-openai scikit-learn langchain-ollama sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdb50ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 documents loaded.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "# Load documents from local files\n",
    "docs_list = []\n",
    "metadata_list = []\n",
    "source_dir = \"./cases\"\n",
    "df = pd.read_csv(\"./metadata.csv\")\n",
    "\n",
    "for filename in glob.glob(source_dir + \"/*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    docs_list.append(filedata)\n",
    "    row = df[(\"./cases/\" + df['filename']) == filename]\n",
    "    if not row.empty:\n",
    "        # Extract metadata as a dictionary\n",
    "        metadata = {\n",
    "            'name': row.iloc[0]['name'],\n",
    "            'year': row.iloc[0]['year'],\n",
    "            'legal_issue': row.iloc[0]['legal_issue']\n",
    "        }\n",
    "        metadata_list.append(metadata)\n",
    "    else:\n",
    "        print(f\"Warning: No metadata found for file {filename}\")\n",
    "        metadata_list.append(None)\n",
    "\n",
    "print(str(len(docs_list)) + \" documents loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5a2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "    ],\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Optional arguments for the text_splitter\n",
    "#    length_function=len,\n",
    "#    is_separator_regex=False,\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.create_documents(docs_list, metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7004577c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1811"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for doc in doc_splits:\n",
    "    sentences.append(doc.page_content)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6202ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "with open(\"open_ai_key.txt\", \"r\") as file:\n",
    "    my_api_key = file.read().strip()\n",
    "#my_api_key = \"your_key_here\"\n",
    "# Create a file in this directory titled \"secret_key.txt\" and add your key.\n",
    "\n",
    "# Create embeddings for documents and store them in a vector store\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings(openai_api_key=my_api_key),\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cace7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the prompt template for the LLM\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are modeled after chief justice John Roberts.\n",
    "    Use the following documents to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Answer in the style of justice John Roberts.\n",
    "    Remain brief with a maximum of 7 sentences:\n",
    "    Question: {question}\n",
    "    Documents: {documents}\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f977a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with the chosen model, set temperature to 0\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf4c2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain combining the prompt template and LLM\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "816e7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG application class\n",
    "class RAGApplication:\n",
    "    def __init__(self, retriever, rag_chain):\n",
    "        self.retriever = retriever\n",
    "        self.rag_chain = rag_chain\n",
    "    def run(self, question):\n",
    "        # Retrieve relevant documents\n",
    "        documents = self.retriever.invoke(question)\n",
    "\n",
    "        # Extract content from retrieved documents\n",
    "        doc_texts = \"\\n\".join([\n",
    "            f\"[METADATA] {doc.metadata}\\n[CONTENT] {doc.page_content}\" for doc in documents\n",
    "        ])\n",
    "        # Get the answer from the language model\n",
    "        answer = self.rag_chain.invoke({\"question\": question, \"documents\": doc_texts})\n",
    "        return answer, doc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4164b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG application\n",
    "rag_application = RAGApplication(retriever, rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4fcc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Should Chevron Deference remain?\n",
      "Answer: <think>\n",
      "Okay, so I need to figure out whether Chevron Deference should remain based on the provided documents and content. Let me start by understanding what Chevron Deference is. From what I remember, Chevron deference is a legal principle that courts use when interpreting agency regulations. It's named after a Supreme Court case where the court applied broad deference to an agency's interpretation of its own laws.\n",
      "\n",
      "Looking at the documents provided, there are several metadata entries with different identifiers and years, but they all pertain to the same case: Loper Bright Enterprises v. Raimondo from 2024, dealing with government agencies. The content seems to discuss whether Chevron Deference should remain in effect.\n",
      "\n",
      "In the first content excerpt, it mentions that under Chevron, ambiguities trigger deference, and both the Government and the dissent argue that Chevron applies even in cases not directly related to an agency's technical expertise. There's a reference to Janus v. State, County, and Municipal Employees, which supports Chevron by stating that all weigh in favor of letting Chevron go.\n",
      "\n",
      "Another content excerpt quotes from Janus, reinforcing the argument for keeping Chevron. However, there's also a mention that Chevron is \"a decaying husk with bold pretensions,\" suggesting it might be losing its relevance or effectiveness. The Court has not deferred to an agency interpretation under Chevron since 2016, as cited in Cuozzo, but Chevron remains on the books.\n",
      "\n",
      "There's a note about lower courts continuing to apply Chevron despite the crumbling precedents, referencing Agostini v. Felton, which indicates that even if Chevron is outdated, courts still use it because of past decisions.\n",
      "\n",
      "Putting this together, the arguments for keeping Chevron are supported by cases like Janus and Cuozzo, but there's also a recognition that Chevron might be losing its standing. The dissent argues that Chevron applies broadly to all agency interpretations, regardless of their technical expertise or context.\n",
      "\n",
      "So, considering these points, I need to decide whether Chevron should remain. On one hand, the majority view seems to support Chevron based on past decisions and the argument that it's a reliable guide for interpreting agency regulations. On the other hand, there are concerns about its relevance and potential overreach in modern contexts.\n",
      "\n",
      "I think the Court would weigh these factors: the established use of Chevron as a useful tool for interpreting agency laws, the arguments from both sides, and any precedents that might indicate a shift in the Court's stance. Since Chevron has been applied consistently despite being less relevant, it might be justified to keep it unless there are compelling reasons to change its application.\n",
      "\n",
      "In conclusion, while there are valid criticisms of Chevron Deference, the established cases suggest it remains a useful principle for interpreting agency regulations. Therefore, Chevron should remain.\n",
      "</think>\n",
      "\n",
      "The Court must consider whether Chevron Deference should continue to apply in light of modern legal contexts and criticisms. While Chevron has been a reliable tool for interpreting agency regulations, recent cases like Cuozzo have shown its use is less frequent. However, the established principle remains useful due to past precedents and consistent application. The balance between respecting historical usage and addressing contemporary concerns suggests Chevron should remain as a guiding principle in agency interpretation.\n",
      "Documents: [METADATA] {'id': '6e4b8416-b2d7-4cdf-bc22-85264d2d7de3', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "[CONTENT] Under that rule, ambiguities of all stripes trigger deference. Indeed, the Government and, seemingly, the dissent continue to defend the proposition that Chevron applies even in cases having little to do with an agency’s technical subject matter expertise. See Brief for Respondents in No. 22–1219, p. 17; post, at 10.\n",
      "[METADATA] {'id': 'ee4d5117-1cfd-484f-b7d9-d1a1dd05d555', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "[CONTENT] (quoting Janus v. State, County, and Municipal Employees, 585 U.S. 878, 917 (2018))—all weigh in favor of letting Chevron go.\n",
      "[METADATA] {'id': '76bde5d5-f506-456d-8999-acfb503d2687', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "[CONTENT] those cases where it might appear to be applicable. See W. Eskridge & L. Baer, The Continuum of Deference: Supreme Court Treatment of Agency Statutory Interpretations From Chevron to Hamdan, 96 Geo. L. J. 1083, 1125 (2008). At this point, all that remains of Chevron is a decaying husk with bold pretensions.\n",
      "[METADATA] {'id': '0be6a931-676e-4d77-9541-ee0dc014556e', 'name': 'Loper Bright Enterprises v. Raimondo, 603 U.S.', 'year': np.int64(2024), 'legal_issue': 'Government Agencies'}\n",
      "[CONTENT] This Court, for its part, has not deferred to an agency interpretation under Chevron since 2016. See Cuozzo, 579 U. S., at 280 (most recent occasion). But Chevron remains on the books. So litigants must continue to wrestle with it, and lower courts—bound by even our crumbling precedents, see Agostini v. Felton, 521 U.S. 203, 238 (1997)—understandably continue to apply it.\n"
     ]
    }
   ],
   "source": [
    "# Run the RAG application\n",
    "question = \"Should Chevron Deference remain?\"\n",
    "answer, doc_texts = rag_application.run(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c1a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
